{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams with pyspark\n",
    "\n",
    "### Create a Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from operator import add\n",
    "\n",
    "filename = \"wiki429MB\"\n",
    "\n",
    "sc = SparkContext(\n",
    "    appName = \"Ngrams with pyspark \" + filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c100.local:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0-cdh6.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Ngrams with pysparkwiki429MB</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=Ngrams with pysparkwiki429MB>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that data is there\n",
    "\n",
    "We are going to use the file `/data/wiki429MB` that has been been previously uploaded to HDFS. The file has size $429$MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 datalab hdfs    428.8 M 2020-02-14 08:54 /data/wiki429MB\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h /data/wiki429MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create RDD from file\n",
    "\n",
    "The second parameter ($80$)  indicates the desired number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textFile is of type: <class 'pyspark.rdd.RDD'>\n",
      "Number of partitions: 80\n"
     ]
    }
   ],
   "source": [
    "textFile = sc.textFile(\"/data/wiki429MB\", 80)\n",
    "print(\"textFile is of type: {}\\nNumber of partitions: {}\".format(type(textFile), textFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "ngrams = textFile \\\n",
    "            .flatMap(lambda x: [x.split()]) \\\n",
    "            .flatMap(lambda x: [tuple(y) for y in zip(*[x[i:] for i in range(n)])]) \\\n",
    "            .map(lambda x: (x, 1)) \\\n",
    "            .reduceByKey(add) \\\n",
    "            .sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('one', 'of', 'the'): 27795\n",
      "('as', 'well', 'as'): 25145\n",
      "('part', 'of', 'the'): 17984\n",
      "('the', 'United', 'States'): 17224\n",
      "('such', 'as', 'the'): 13886\n",
      "('the', 'end', 'of'): 13878\n",
      "('a', 'number', 'of'): 12986\n",
      "('in', 'the', 'United'): 11760\n",
      "('known', 'as', 'the'): 10172\n",
      "('end', 'of', 'the'): 9842\n"
     ]
    }
   ],
   "source": [
    "for (ngram, count) in ngrams.take(10):\n",
    "    print(\"%s: %i\" % (ngram, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop context\n",
    "\n",
    "Stop the current Spark context to free resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
