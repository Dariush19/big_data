{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode columns in csv file\n",
    "\n",
    "I'm given a CSV file containing strings and I want to convert the characters to numeric values. I want to use different encodings of the characters on different columns or groups of columns. \n",
    "\n",
    "For example, let's say I have two encodings:\n",
    " - in encoding __A__ I want to encode the character `a` with the number `1`, the character `b` with `2`, and `c` with `3`\n",
    " - in encoding __B__ I want to encode the character `a` with the number `2`, the character `b` with `3`, and `c` with `1`\n",
    " \n",
    " If I use encoding `A` to transform all columns in table\n",
    " \n",
    "| c1| c2 |\n",
    "|-----|-----|\n",
    "| a | a|\n",
    "| b | b|\n",
    "| c | b|\n",
    "\n",
    "I obtain\n",
    "\n",
    "| c1_enc| c2_enc |\n",
    "|-----|-----|\n",
    "| 1 | 1|\n",
    "| 2 | 2|\n",
    "| 3 | 2|\n",
    "\n",
    "If `col1` is encoded with `A` and `col2` is encoded with `B` then the table becomes\n",
    "\n",
    "| c1_enc| c2_enc |\n",
    "|-----|-----|\n",
    "| 1 | 2|\n",
    "| 2 | 3|\n",
    "| 3 | 3|\n",
    "\n",
    "The challenge is to carry out this transformation for tables with thousands of columns and millions of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Spark session and load data\n",
    "\n",
    "SparkContext allows me to access Dataframes, change Spark configuration, cancel a job, get status of a job, etc.\n",
    "Load CSV file using the header as column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Encode multiple columns\") \\\n",
    "     .getOrCreate()\n",
    "        \n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.csv(\"data-1600cols.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('hive.metastore.warehouse.dir', 'file:/home/jovyan/work/spark-warehouse/'),\n",
       " ('spark.app.name', 'Encode multiple columns'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1566927012283'),\n",
       " ('spark.driver.port', '43689'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '172.17.0.7')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size of the dataframe (number of rows and columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1000\n",
      "Number of columns: 1600\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows: {}\\nNumber of columns: {}'.format(df.count(),len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataframe contains any nulls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.V2.isNull()).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a couple of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| V1| V2| V3|\n",
      "+---+---+---+\n",
      "|  j|  n|  d|\n",
      "|  d|  n|  w|\n",
      "|  p|  h|  a|\n",
      "|  b|  h|  e|\n",
      "|  z|  x|  u|\n",
      "|  b|  e|  v|\n",
      "|  y|  t|  x|\n",
      "|  i|  r|  e|\n",
      "|  x|  e|  g|\n",
      "|  l|  j|  z|\n",
      "|  l|  v|  l|\n",
      "|  z|  n|  h|\n",
      "|  s|  m|  c|\n",
      "|  g|  m|  f|\n",
      "|  i|  p|  n|\n",
      "|  i|  f|  b|\n",
      "|  u|  n|  j|\n",
      "|  s|  o|  e|\n",
      "|  k|  y|  c|\n",
      "|  h|  b|  i|\n",
      "+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('V1','V2','V3').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach\n",
    "\n",
    "Using the `translate` function from `pyspark.sql` and adding a new column with `withColumn` at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| c1| c2|\n",
      "+---+---+\n",
      "|  a|  a|\n",
      "|  b|  b|\n",
      "|  c|  b|\n",
      "+---+---+\n",
      "\n",
      "+---+---+------+------+\n",
      "| c1| c2|c1_enc|c2_enc|\n",
      "+---+---+------+------+\n",
      "|  a|  a|     1|     1|\n",
      "|  b|  b|     2|     2|\n",
      "|  c|  b|     3|     2|\n",
      "+---+---+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "test_df = sqlContext.createDataFrame([('a', 'a'), ('b', 'b'), ('c', 'b')], ['c1', 'c2'])\n",
    "test_df.show()\n",
    "\n",
    "chars = \"abc\"\n",
    "A = \"123\" # encoding A\n",
    "B = \"231\" # encoding B\n",
    "\n",
    "\n",
    "for col_name in [\"c1\", \"c2\"]:  \n",
    "    test_df = test_df.withColumn(col_name+'_enc', f.translate(f.col(col_name), \"abcd\", A))\n",
    "\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out this approach on the big dataframe, applying the function to a few columns. I define two random encodings, `encodingA` and `encodingB` and apply each encoding to two different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodings:\n",
      "abcdefghijklmnopqrstuvwxyz\n",
      "84909340662170830129865816\n",
      "03946914819742444812351068\n",
      "--------------------------\n",
      "+---+---+---+---+------+------+------+------+\n",
      "| V1| V2| V3| V4|V1_enc|V2_enc|V3_enc|V4_enc|\n",
      "+---+---+---+---+------+------+------+------+\n",
      "|  j|  n|  d|  m|     6|     2|     0|     4|\n",
      "|  d|  n|  w|  y|     0|     2|     5|     6|\n",
      "|  p|  h|  a|  h|     3|     4|     8|     4|\n",
      "|  b|  h|  e|  t|     4|     4|     9|     2|\n",
      "|  z|  x|  u|  d|     6|     0|     8|     4|\n",
      "|  b|  e|  v|  j|     4|     6|     6|     1|\n",
      "|  y|  t|  x|  w|     1|     2|     8|     1|\n",
      "|  i|  r|  e|  q|     6|     8|     9|     4|\n",
      "|  x|  e|  g|  s|     8|     6|     4|     1|\n",
      "|  l|  j|  z|  h|     1|     1|     6|     4|\n",
      "|  l|  v|  l|  w|     1|     5|     1|     1|\n",
      "|  z|  n|  h|  z|     6|     2|     0|     8|\n",
      "|  s|  m|  c|  z|     2|     4|     9|     8|\n",
      "|  g|  m|  f|  j|     4|     4|     3|     1|\n",
      "|  i|  p|  n|  h|     6|     4|     0|     4|\n",
      "|  i|  f|  b|  r|     6|     9|     4|     8|\n",
      "|  u|  n|  j|  p|     8|     2|     6|     4|\n",
      "|  s|  o|  e|  f|     2|     4|     9|     9|\n",
      "|  k|  y|  c|  c|     2|     6|     9|     9|\n",
      "|  h|  b|  i|  p|     0|     3|     6|     4|\n",
      "+---+---+---+---+------+------+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "# set a raneom seed\n",
    "random.seed(30)\n",
    "\n",
    "chars = string.ascii_lowercase\n",
    "encodingA = ''.join(random.choice(string.digits) for i in range(len(chars)))\n",
    "encodingB = ''.join(random.choice(string.digits) for i in range(len(chars)))\n",
    "\n",
    "print(\"Encodings:\")\n",
    "print(chars)\n",
    "print(encodingA)\n",
    "print(encodingB)\n",
    "print(\"-\"*26)\n",
    "new_df=df\n",
    "\n",
    "for col_name in [\"V1\", \"V3\"]:  # apply encodingA to columns V1, V3\n",
    "    new_df=new_df.withColumn(col_name+'_enc',f.translate(f.col(col_name), chars, encodingA))\n",
    "for col_name in [\"V2\", \"V4\"]:  # apply encodingB to columns V2, V4\n",
    "    new_df=new_df.withColumn(col_name+'_enc',f.translate(f.col(col_name), chars, encodingB))\n",
    "    \n",
    "new_df.select(\"V1\",\"V2\",\"V3\",\"V4\", \"V1_enc\", \"V2_enc\", \"V3_enc\", \"V4_enc\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply encodings to 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "| V1| V2| V3| V4|\n",
      "+---+---+---+---+\n",
      "|  6|  2|  0|  4|\n",
      "|  0|  2|  5|  6|\n",
      "|  3|  4|  8|  4|\n",
      "+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df=df\n",
    "\n",
    "for col_name in [\"V1\", \"V3\"]:  # apply encodingA to columns V1, V2\n",
    "    new_df = new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingA))\n",
    "for col_name in [\"V2\", \"V4\"]:  # apply encodingB to columns V3, V4\n",
    "    new_df = new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingB))\n",
    "    \n",
    "new_df.select(\"V1\",\"V2\",\"V3\",\"V4\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:\n",
    "\n",
    "| V1 | V2 | V3 | V4\n",
    "|-----|-----|-----|\n",
    "| 9 | 2 | 2 | 8 |\n",
    "| 0 | 1 | 0 | 1 |\n",
    "| 5 | 0 | 0 | 6 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying encoding to thousands of rows the previous approach is too slow. The reason is that I'm writing a new dataframe after each tranformation.\n",
    "\n",
    "Split columns in even and odd, apply two different encodings to each set of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V2', 'V4']\n",
      "['V1', 'V3']\n",
      "+---+---+---+---+\n",
      "| V1| V2| V3| V4|\n",
      "+---+---+---+---+\n",
      "|  6|  2|  0|  4|\n",
      "|  0|  2|  5|  6|\n",
      "|  3|  4|  8|  4|\n",
      "+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_e = [\"V\"+str(i) for i in range(2,5,2)]\n",
    "cols_o = [\"V\"+str(i) for i in range(1,4,2)]\n",
    "\n",
    "print(cols_e)\n",
    "print(cols_o)\n",
    "\n",
    "new_df=df\n",
    "\n",
    "# works with a few columns (4 in total in this example) but too slow for thousands of columns\n",
    "for col_name in cols_o:  # apply encodingA to columns with even numbers\n",
    "    new_df=new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingA))\n",
    "for col_name in cols_e:  # apply encodingB to odd columns \n",
    "    new_df=new_df.withColumn(col_name,f.translate(f.col(col_name), chars, encodingB))\n",
    "    \n",
    "new_df.select([\"V\"+str(i) for i in range(1,5)]).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach\n",
    "Using `udf` (user-defined functions). Avoiding `withColumn` and using `select` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V1', 'V3', 'V5']\n",
      "+---+---+---+------+------+------+\n",
      "| V1| V3| V5|V1_enc|V3_enc|V5_enc|\n",
      "+---+---+---+------+------+------+\n",
      "|  j|  d|  s|     6|     0|     2|\n",
      "|  d|  w|  l|     0|     5|     1|\n",
      "|  p|  a|  w|     3|     8|     5|\n",
      "|  b|  e|  x|     4|     9|     8|\n",
      "|  z|  u|  b|     6|     8|     4|\n",
      "|  b|  v|  u|     4|     6|     8|\n",
      "|  y|  x|  z|     1|     8|     6|\n",
      "|  i|  e|  k|     6|     9|     2|\n",
      "|  x|  g|  s|     8|     4|     2|\n",
      "|  l|  z|  l|     1|     6|     1|\n",
      "+---+---+---+------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "# define an encoding as a list of two strings of equal length\n",
    "\n",
    "o = [\"abcdefghijklmnopqrstuvwxyz\", encodingA]\n",
    "\n",
    "def enc(*a):\n",
    "    # encode string s with encoding o\n",
    "    s=a[0]\n",
    "    for i in range(len(o[0])): \n",
    "      if s==o[0][i]:      \n",
    "          return o[1][i]\n",
    "    return s\n",
    "\n",
    "# create udf\n",
    "encode_udf = udf(enc, StringType())\n",
    "\n",
    "cols_o = [\"V\"+str(i) for i in range(7) if i%2==1]\n",
    "print(cols_o)\n",
    "\n",
    "(\n",
    "df.select(\"V1\",\"V3\",\"V5\", \n",
    "           encode_udf(\"V1\").alias(\"V1_enc\"),\n",
    "           encode_udf(\"V3\").alias(\"V3_enc\"),\n",
    "           encode_udf(\"V5\").alias(\"V5_enc\"))\n",
    "    .show(10) \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now encode all even and odd numbered columns with `encodingA` and `encodingB`, respectively using `select`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "|V1_enc|V3_enc|V5_enc|V7_enc|V9_enc|V11_enc|V13_enc|V15_enc|V17_enc|V19_enc|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "|     6|     0|     2|     6|     9|      8|      2|      2|      3|      6|\n",
      "|     0|     5|     1|     8|     0|      2|      9|      6|      8|      2|\n",
      "|     3|     8|     5|     4|     8|      3|      9|      0|      2|      9|\n",
      "|     4|     9|     8|     0|     9|      0|      9|      2|      8|      0|\n",
      "|     6|     8|     4|     0|     9|      2|      8|      6|      6|      6|\n",
      "|     4|     6|     8|     5|     8|      6|      5|      6|      6|      4|\n",
      "|     1|     8|     6|     0|     4|      8|      4|      5|      5|      1|\n",
      "|     6|     9|     2|     5|     8|      8|      5|      4|      0|      1|\n",
      "|     8|     4|     2|     2|     2|      2|      9|      7|      8|      0|\n",
      "|     1|     6|     1|     9|     6|      2|      6|      1|      1|      2|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to 50 columns\n",
    "new_df=df.select([encode_udf(\"V\"+str(i)).alias(\"V\"+str(i)+\"_enc\") for i in range(1,100,2)])\n",
    "new_df.select([\"V\"+str(i)+\"_enc\" for i in range(1,21,2)]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "|V1_enc|V3_enc|V5_enc|V7_enc|V9_enc|V11_enc|V13_enc|V15_enc|V17_enc|V19_enc|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "|     6|     0|     2|     6|     9|      8|      2|      2|      3|      6|\n",
      "|     0|     5|     1|     8|     0|      2|      9|      6|      8|      2|\n",
      "|     3|     8|     5|     4|     8|      3|      9|      0|      2|      9|\n",
      "|     4|     9|     8|     0|     9|      0|      9|      2|      8|      0|\n",
      "|     6|     8|     4|     0|     9|      2|      8|      6|      6|      6|\n",
      "|     4|     6|     8|     5|     8|      6|      5|      6|      6|      4|\n",
      "|     1|     8|     6|     0|     4|      8|      4|      5|      5|      1|\n",
      "|     6|     9|     2|     5|     8|      8|      5|      4|      0|      1|\n",
      "|     8|     4|     2|     2|     2|      2|      9|      7|      8|      0|\n",
      "|     1|     6|     1|     9|     6|      2|      6|      1|      1|      2|\n",
      "+------+------+------+------+------+-------+-------+-------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to 100 columns\n",
    "new_df=df.select([encode_udf(\"V\"+str(i)).alias(\"V\"+str(i)+\"_enc\") for i in range(1,201,2)])\n",
    "new_df.select([\"V\"+str(i)+\"_enc\" for i in range(1,21,2)]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|V381_enc|V383_enc|V385_enc|V387_enc|V389_enc|V391_enc|V393_enc|V395_enc|V397_enc|V399_enc|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|       4|       8|       6|       8|       9|       6|       8|       8|       6|       8|\n",
      "|       6|       4|       8|       4|       1|       6|       6|       0|       9|       6|\n",
      "|       9|       6|       8|       8|       2|       4|       0|       5|       9|       1|\n",
      "|       3|       6|       6|       6|       1|       9|       0|       0|       4|       4|\n",
      "|       6|       0|       1|       0|       1|       1|       2|       9|       2|       8|\n",
      "|       0|       4|       3|       4|       8|       4|       8|       6|       2|       1|\n",
      "|       1|       1|       8|       2|       6|       2|       1|       5|       1|       4|\n",
      "|       4|       4|       1|       0|       1|       2|       4|       8|       8|       8|\n",
      "|       2|       0|       8|       6|       8|       0|       6|       6|       7|       6|\n",
      "|       1|       4|       0|       9|       8|       6|       6|       0|       2|       9|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to 400 columns\n",
    "new_df=df.select([encode_udf(\"V\"+str(i)).alias(\"V\"+str(i)+\"_enc\") for i in range(1,401,2)])\n",
    "new_df.select([\"V\"+str(i)+\"_enc\" for i in range(381,401,2)]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|V781_enc|V783_enc|V785_enc|V787_enc|V789_enc|V791_enc|V793_enc|V795_enc|V797_enc|V799_enc|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "|       2|       8|       0|       9|       9|       3|       0|       2|       6|       8|\n",
      "|       0|       9|       3|       5|       9|       9|       0|       1|       5|       9|\n",
      "|       6|       4|       8|       8|       3|       8|       5|       0|       3|       0|\n",
      "|       4|       7|       0|       6|       2|       1|       0|       6|       0|       4|\n",
      "|       3|       3|       7|       6|       8|       8|       6|       4|       0|       6|\n",
      "|       8|       8|       1|       8|       8|       4|       4|       5|       4|       2|\n",
      "|       9|       0|       8|       2|       0|       0|       6|       0|       6|       2|\n",
      "|       1|       2|       5|       6|       6|       9|       2|       7|       6|       0|\n",
      "|       0|       8|       2|       0|       8|       1|       7|       9|       8|       1|\n",
      "|       7|       0|       4|       2|       4|       8|       1|       6|       6|       4|\n",
      "+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to all odd columns\n",
    "\n",
    "new_df = df.select([encode_udf(\"V\"+str(i)).alias(\"V\"+str(i)+\"_enc\") for i in range(1,801,2)])\n",
    "\n",
    "new_df.select([\"V\"+str(i)+\"_enc\" for i in range(781,801,2)]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to apply different udfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+--------+--------+--------+--------+--------+--------+\n",
      "|V1_enc|V2_enc|V3_enc|V4_enc|V795_enc|V796_enc|V797_enc|V798_enc|V799_enc|V800_enc|\n",
      "+------+------+------+------+--------+--------+--------+--------+--------+--------+\n",
      "|     6|     2|     0|     4|       2|       4|       6|       4|       8|       8|\n",
      "|     0|     2|     5|     6|       1|       5|       5|       6|       9|       3|\n",
      "|     3|     4|     8|     4|       0|       0|       3|       2|       0|       6|\n",
      "|     4|     4|     9|     2|       6|       4|       0|       4|       4|       8|\n",
      "|     6|     0|     8|     4|       4|       4|       0|       6|       6|       4|\n",
      "|     4|     6|     6|     1|       5|       5|       4|       9|       2|       1|\n",
      "|     1|     2|     8|     1|       0|       4|       6|       8|       2|       4|\n",
      "|     6|     8|     9|     4|       7|       9|       6|       4|       0|       1|\n",
      "|     8|     6|     4|     1|       9|       8|       8|       8|       1|       3|\n",
      "|     1|     1|     6|     4|       6|       6|       6|       3|       4|       8|\n",
      "+------+------+------+------+--------+--------+--------+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = [\"abcdefghijklmnopqrstuvwxyz\", encodingA]\n",
    "e = [\"abcdefghijklmnopqrstuvwxyz\", encodingB]\n",
    "\n",
    "# define two encoding functions\n",
    "\n",
    "def enc1(*a):\n",
    "    # encode string s with encoding o\n",
    "    s=a[0]\n",
    "    for i in range(len(o[0])): \n",
    "      if s==o[0][i]:      \n",
    "          return o[1][i]\n",
    "    return s\n",
    "\n",
    "def enc2(*a):\n",
    "    # encode string s with encoding e\n",
    "    s=a[0]\n",
    "    for i in range(len(e[0])): \n",
    "      if s==e[0][i]:      \n",
    "          return e[1][i]\n",
    "    return s\n",
    "\n",
    "# create udfs\n",
    "encode_udf1 = udf(enc1, StringType())\n",
    "encode_udf2 = udf(enc2, StringType())\n",
    "\n",
    "new_df = df.select([encode_udf1(\"V\"+str(i)).alias(\"V\"+str(i)+\"_enc\") for i in range(1,800,2)]+\n",
    "                  [encode_udf2(\"V\"+str(i)).alias(\"V\"+str(i)+\"_enc\") for i in range(2,801,2)])\n",
    "new_df.select([\"V\"+str(i)+\"_enc\" for i in range(1,5)]+[\"V\"+str(i)+\"_enc\" for i in range(795,801)]).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              total        used        free      shared  buff/cache   available\r\n",
      "Mem:            62G        7.1G         25G         72K         29G         52G\r\n",
      "Swap:            0B          0B          0B\r\n"
     ]
    }
   ],
   "source": [
    "!free -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:          x86_64\r\n",
      "CPU op-mode(s):        32-bit, 64-bit\r\n",
      "Byte Order:            Little Endian\r\n",
      "CPU(s):                8\r\n",
      "On-line CPU(s) list:   0-7\r\n",
      "Thread(s) per core:    2\r\n",
      "Core(s) per socket:    4\r\n",
      "Socket(s):             1\r\n",
      "NUMA node(s):          1\r\n",
      "Vendor ID:             GenuineIntel\r\n",
      "CPU family:            6\r\n",
      "Model:                 85\r\n",
      "Model name:            Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz\r\n",
      "Stepping:              4\r\n",
      "CPU MHz:               3099.113\r\n",
      "BogoMIPS:              5000.00\r\n",
      "Hypervisor vendor:     KVM\r\n",
      "Virtualization type:   full\r\n",
      "L1d cache:             32K\r\n",
      "L1i cache:             32K\r\n",
      "L2 cache:              1024K\r\n",
      "L3 cache:              33792K\r\n",
      "NUMA node0 CPU(s):     0-7\r\n",
      "Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke\r\n"
     ]
    }
   ],
   "source": [
    "!lscpu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframe to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved out20190827202026.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "new_df.write.csv('out'+timestamp+'.csv', sep=',')\n",
    "print('saved out{}.csv'.format(timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to CSV with headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved out20190827202037.csv\n"
     ]
    }
   ],
   "source": [
    "timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "new_df.write.csv('out'+timestamp+'.csv', sep=',', header = True)\n",
    "print('saved out{}.csv'.format(timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls out*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
